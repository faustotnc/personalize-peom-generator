{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import unidecode\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Prevent kernel from dying\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data\n",
    "labeledPfd = pd.read_csv(\"./datasets/LabeledPoetryFoundationPoems.csv\")\n",
    "\n",
    "\n",
    "# Filters the poems by category\n",
    "def get_poems_by_category(category):\n",
    "    data = []\n",
    "    for poem, emotion in zip(labeledPfd[\"poem\"], labeledPfd[\"emotion\"]):\n",
    "        if emotion == category:\n",
    "            data.append(poem)\n",
    "    return data\n",
    "\n",
    "\n",
    "# The available poem categories\n",
    "all_categories = [\"joy\", \"trust\", \"sadness\", \"anticipation\", \"fear\", \"anger\", \"disgust\", \"surprise\"]\n",
    "# The number of categories we have\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "# Gets the poems in each category\n",
    "poems = {\n",
    "    \"joy\": get_poems_by_category(\"joy\"),\n",
    "    \"trust\": get_poems_by_category(\"trust\"),\n",
    "    \"sadness\": get_poems_by_category(\"sadness\"),\n",
    "    \"anticipation\": get_poems_by_category(\"anticipation\"),\n",
    "    \"fear\": get_poems_by_category(\"fear\"),\n",
    "    \"anger\": get_poems_by_category(\"anger\"),\n",
    "    \"disgust\": get_poems_by_category(\"disgust\"),\n",
    "    \"surprise\": get_poems_by_category(\"surprise\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with only 500 poems.\n",
    "# When trying to train with the entrie dataset (~4000 poems)\n",
    "# the kernel dies before completing the first epoch.\n",
    "text = list(poems[\"joy\"][:500])\n",
    "\n",
    "def joinStrings(text):\n",
    "    return ' '.join(string for string in text)\n",
    "text = joinStrings(text)\n",
    "len(text.split()) # Number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop = set(nltk.corpus.stopwords.words('english'))\n",
    "# exclude = set(string.punctuation) \n",
    "# lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "# def clean(doc):\n",
    "#         stop_free = \" \".join([i for i in doc.split() if i not in stop])\n",
    "#         punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "#         normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "#         return normalized\n",
    "\n",
    "clean_data = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates trigram word embeddings\n",
    "# with the data. For the sentence:\n",
    "# \"I party with my friends on the weekedns\"\n",
    "# \"with\", and \"my\" will be the context for\n",
    "# the word \"friends\". An example is printed bellow\n",
    "embeddings = [ ([trigram[0], trigram[1]], trigram[2]) for trigram in ngrams(clean_data, 3)]\n",
    "chunk_len = len(embeddings)\n",
    "print(embeddings[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the vocabulary\n",
    "vocab = set(clean_data)\n",
    "voc_len = len(vocab)\n",
    "\n",
    "# Encodes the position of each word in the vocabulary\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the input and target vectors\n",
    "inp, tar = ([], [])\n",
    "for context, target in embeddings:\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        inp.append(context_idxs)\n",
    "        targ = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
    "        tar.append(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # We are using the GRU method to train the model\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers,batch_first=True,\n",
    "                          bidirectional=False)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "hidden_size = 120 # 120 nodes on each hidden layer \n",
    "n_layers = 2 # two hidden layers\n",
    "lr = 0.1 # learning rate\n",
    "\n",
    "decoder = RNN(voc_len, hidden_size, voc_len, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(inp, tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prime_str='this love', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        prime_input = torch.tensor([word_to_ix[w] for w in prime_str.split()], dtype=torch.long)\n",
    "        inp = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_ix.keys())[list(word_to_ix.values()).index(top_i)]\n",
    "        prime_str += \" \" + predicted_word\n",
    "\n",
    "    return prime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_poem = predict(prime_str='this love', predict_len=100, temperature=1)\n",
    "\n",
    "print(generated_poem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
