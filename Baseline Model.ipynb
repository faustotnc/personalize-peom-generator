{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustotnc/.pyenv/versions/3.8.5/lib/python3.8/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import unidecode\n",
    "import time, math\n",
    "import random\n",
    "\n",
    "# Used by the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "# Prevent kernel from dying\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data\n",
    "labeledPfd = pd.read_csv(\"./datasets/LabeledPoetryFoundationPoems.csv\")\n",
    "labeledPfd = labeledPfd.sample(frac=1) # randomizes the data\n",
    "\n",
    "# Filters the poems by category\n",
    "def get_poems_by_category(category):\n",
    "    data = []\n",
    "    for poem, emotion in zip(labeledPfd[\"poem\"], labeledPfd[\"emotion\"]):\n",
    "        if emotion == category:\n",
    "            data.append(poem)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Gets the poems in each category\n",
    "poems = {\n",
    "    \"joy\": get_poems_by_category(\"joy\"),\n",
    "    \"trust\": get_poems_by_category(\"trust\"),\n",
    "    \"sadness\": get_poems_by_category(\"sadness\"),\n",
    "    \"anticipation\": get_poems_by_category(\"anticipation\"),\n",
    "    \"fear\": get_poems_by_category(\"fear\"),\n",
    "    \"anger\": get_poems_by_category(\"anger\"),\n",
    "    \"disgust\": get_poems_by_category(\"disgust\"),\n",
    "    \"surprise\": get_poems_by_category(\"surprise\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.outputs[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \" .,;?\\\"\\n\"\n",
    "class DataProcessor:\n",
    "    def __init__(self, dataset, batch_size=48):\n",
    "        full_data = list(dataset)\n",
    "        \n",
    "        # first 20% as test data\n",
    "        twentyPercent = math.ceil(len(full_data) * 0.2)\n",
    "        self.test_set = full_data[:twentyPercent]\n",
    "        # last 80% as train data\n",
    "        eigtyPercent = math.floor(len(full_data) * 0.8)\n",
    "        full_data = full_data[-eigtyPercent:]\n",
    "\n",
    "        def joinStrings(text):\n",
    "            return '\\n'.join(string for string in text)\n",
    "        full_data = joinStrings(full_data)\n",
    "\n",
    "        # Clean the data\n",
    "        # Removes punctuation from the dataset\n",
    "        self.clean_data = [word for word in nltk.word_tokenize(full_data) if (word.isalnum() or word in punctuation)]\n",
    "        # print(\"Number of tokens:\", len(clean_data)) # Number of tokens\n",
    "        # print(clean_data[:10])\n",
    "\n",
    "        # Extracts the vocabulary\n",
    "        self.vocab = set(self.clean_data)\n",
    "        self.voc_len = len(self.vocab)\n",
    "\n",
    "        # Encodes the position of each word in the vocabulary\n",
    "        self.word_to_ix = {word: i for i, word in enumerate(self.vocab)}\n",
    "\n",
    "        # Generates trigram word embeddings\n",
    "        # with the data. For the sentence:\n",
    "        # \"I party with my friends on the weekends\"\n",
    "        # \"with\", and \"my\" will be the context for\n",
    "        # the word \"friends\". An example is printed bellow\n",
    "        self.embeddings = [\n",
    "            ([trigram[0], trigram[1]], trigram[2]) for trigram in ngrams(self.clean_data, 3)\n",
    "        ]\n",
    "        self.num_embeddings = len(self.embeddings)\n",
    "        \n",
    "        # Training and Testing sets ready to be used\n",
    "        self.train_set = BatchGenerator(*self.inp_tar_pairs())\n",
    "        \n",
    "    def inp_tar_pairs(self):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        \n",
    "        for context, target in self.embeddings:\n",
    "            # 2dim tensor with the positions of the context letters\n",
    "            context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n",
    "            inputs.append(context_idxs)\n",
    "\n",
    "            # 1dim tensor with the position of the target letter\n",
    "            targ = torch.tensor([self.word_to_ix[target]], dtype=torch.long)\n",
    "            targets.append(targ)\n",
    "            \n",
    "        return inputs, targets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Recurrent Neural Network (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # We are using the GRU method to train the model\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size * 2,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            batch_first = True, # x: (num_embeddings, context_size, input_size)\n",
    "            bidirectional = False\n",
    "        )\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_vect, hidden):\n",
    "        input_vect = self.encoder(input_vect.view(1, -1)) # flattens the input vector\n",
    "        output, hidden = self.gru(input_vect.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "    \n",
    "    \n",
    "    def trainer(self, model, train_set):\n",
    "        \"\"\"\n",
    "        Trains the model with the data for\n",
    "        the equivalent of 1 epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initializes the hidden state\n",
    "        hidden = self.init_hidden()\n",
    "\n",
    "        # Initializes the gradients and the loss\n",
    "        self.zero_grad()\n",
    "        loss = 0\n",
    "\n",
    "        # Trains the neural network over all\n",
    "        # the character embeddings\n",
    "        for inp, tar in train_set:\n",
    "            # The model taken in a context tensor, and\n",
    "            # the previous hidden state to predict an\n",
    "            # output, and compute a new hidden state\n",
    "            output, hidden = self.forward(inp, hidden)\n",
    "\n",
    "            # The loss is computed using the predicted output\n",
    "            # and the target (expected output)\n",
    "            loss += model.criterion(output, tar)\n",
    "\n",
    "        # Propagates the loss backwards\n",
    "        # through the network\n",
    "        loss.backward()\n",
    "        model.model_optimizer.step()\n",
    "\n",
    "        # Returns the loss of the network\n",
    "        return loss.data.item() / model.data.num_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemModel:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "        # Model Hyperparameters\n",
    "        self.n_epochs      = 10\n",
    "        self.print_every   = 1\n",
    "        self.plot_every    = 1\n",
    "        self.hidden_size   = 120\n",
    "        self.n_layers      = 3\n",
    "        self.learning_rate = 0.01\n",
    "        self.batch_size    = 32\n",
    "        \n",
    "        # RNN Initialization\n",
    "        self.model = RNN(data.voc_len, self.hidden_size, data.voc_len, self.n_layers)\n",
    "        self.model_optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    def train(self, plot_loss=True):\n",
    "        total_time_start = time.time()\n",
    "        all_losses = []\n",
    "        loss_avg = 0\n",
    "\n",
    "        # Converts the execution time\n",
    "        # to a human-readible format\n",
    "        def time_since(since):\n",
    "            s = time.time() - since\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "        \n",
    "        print(\"Started training...\")\n",
    "            \n",
    "        # Trains the model.\n",
    "        # n_epochs determines how many times we\n",
    "        # will show the same data to the network.\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            start = time.time()\n",
    "            \n",
    "            # Divides the entire dataset into [batch_size] batches\n",
    "            training_generator = torch.utils.data.DataLoader(self.data.train_set, \n",
    "                batch_size = self.batch_size,\n",
    "                shuffle = True,\n",
    "            )\n",
    "            training_generator = iter(training_generator)\n",
    "            \n",
    "            # Only trains using [batch_size] of the generated batches.\n",
    "            # So if there were a total of 500 batches, and our batch_size\n",
    "            # equals 20, then it will only train with 20 batches of 20 pairs\n",
    "            batch_index = 0\n",
    "            for index in range(self.batch_size):\n",
    "                batch_index += 1\n",
    "                print(\"Running batch #\" + str(batch_index) + \" out of \" + str(self.batch_size), end=\"\\r\")\n",
    "                \n",
    "                # Trains the model for the\n",
    "                # current epoch\n",
    "                inp, tar = training_generator.next()\n",
    "                loss = self.model.trainer(self, zip(inp, tar))  \n",
    "                loss_avg += loss\n",
    "\n",
    "            # Logs out the epoch execution time\n",
    "            if epoch % self.print_every == 0:\n",
    "                print('Epoch #%d (%d%%) [Exc. Time: %s, Loss: %.4f]' % (\n",
    "                    epoch, epoch / self.n_epochs * 100, time_since(start), loss\n",
    "                ))\n",
    "\n",
    "            # Saves the epoch execution time for later plotting\n",
    "            if epoch % self.plot_every == 0:\n",
    "                all_losses.append(loss_avg / self.plot_every)\n",
    "                loss_avg = 0\n",
    "\n",
    "        # Prints the total time taken by training the model\n",
    "        print(\"\\nTotal training time:\", time_since(total_time_start))\n",
    "        \n",
    "        # Plots the loss\n",
    "        if plot_loss:\n",
    "            plt.figure()\n",
    "            plt.plot(all_losses)\n",
    "        \n",
    "        \n",
    "    def generate(self, prime_str, predict_len=100, temperature=0.8):\n",
    "        hidden = self.model.init_hidden()\n",
    "\n",
    "        for p in range(predict_len):\n",
    "            prime_input = torch.tensor([self.data.word_to_ix[w] for w in prime_str.split()], dtype=torch.long)\n",
    "            inp = prime_input[-2:] #last two words as input\n",
    "            output, hidden = self.model.forward(inp, hidden)\n",
    "\n",
    "            # Sample from the network as a multinomial distribution\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "            # Add predicted word to string and use as next input\n",
    "            predicted_word = list(self.data.word_to_ix.keys())[list(self.data.word_to_ix.values()).index(top_i)]\n",
    "            prime_str += \" \" + predicted_word\n",
    "\n",
    "        return prime_str\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        rouge = Rouge()\n",
    "        \n",
    "        # Selects the first 10 poems of the\n",
    "        # test set to test the model\n",
    "        first_10 = self.data.test_set[:10]\n",
    "        \n",
    "        # cleans a peom\n",
    "        def clean_poem(poem):\n",
    "            return [word for word in nltk.word_tokenize(poem) if (word.isalnum() and word in self.data.vocab)]\n",
    "        \n",
    "        # list of poems with clean data as hypothesis for ROUGE\n",
    "        rouge_hyp = [\" \".join(clean_poem(poem)) for poem in first_10]\n",
    "        rouge_refs = []\n",
    "        \n",
    "        # list of lists of poems with clean data as hypothesis for BLEU\n",
    "        bleu_references = [clean_poem(poem) for poem in first_10]\n",
    "        bleu_candidates = []\n",
    "\n",
    "        for i in range(10):\n",
    "            first_two_words = \" \".join(rouge_hyp[i].split(\" \")[:2])\n",
    "            gen_poem = self.generate(first_two_words)\n",
    "            gen_poem = [word for word in nltk.word_tokenize(gen_poem)]\n",
    "            \n",
    "            # for rouge\n",
    "            rouge_refs.append(\" \".join(gen_poem))\n",
    "            \n",
    "            # for bleu\n",
    "            bleu_candidates.append(gen_poem)\n",
    "\n",
    "        rouge_scores = rouge.get_scores(rouge_hyp, rouge_refs, avg=True)\n",
    "        print(\"ROUGE SCORE:\", rouge_scores)\n",
    "        \n",
    "        bleu_score = corpus_bleu(bleu_references, bleu_candidates)\n",
    "        print(\"\\nBLEU SCORE:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model for the \"Joy\" Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "joyData = DataProcessor(poems[\"joy\"][:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training...\n",
      "Epoch #1 (10%) [Exc. Time: 0m 20s, Loss: 0.0007]\n",
      "Epoch #2 (20%) [Exc. Time: 0m 20s, Loss: 0.0006]\n",
      "Epoch #3 (30%) [Exc. Time: 0m 20s, Loss: 0.0006]\n",
      "Epoch #4 (40%) [Exc. Time: 0m 20s, Loss: 0.0007]\n",
      "Epoch #5 (50%) [Exc. Time: 0m 20s, Loss: 0.0006]\n",
      "Epoch #6 (60%) [Exc. Time: 0m 20s, Loss: 0.0007]\n",
      "Epoch #7 (70%) [Exc. Time: 0m 20s, Loss: 0.0006]\n",
      "Epoch #8 (80%) [Exc. Time: 0m 20s, Loss: 0.0005]\n",
      "Epoch #9 (90%) [Exc. Time: 0m 20s, Loss: 0.0006]\n",
      "Epoch #10 (100%) [Exc. Time: 0m 20s, Loss: 0.0006]\n",
      "\n",
      "Total training time: 3m 24s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = PoemModel(joyData)\n",
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh boy , a thou , goodies canal this she , When , The its they dancers thou the window . in she , be , I , behind There too with would . a falls , fox not , ? . , , . . seemed skin , power love a my the remarkable moonshine of , a , , i all while wine , . How godly , almost , The , and , music call ; snow is . With their thou , by . , d I were a by the they s dark of , The ,'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate(\"Oh boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE SCORE: {'rouge-1': {'f': 0.1635004547866158, 'p': 0.17920632873518744, 'r': 0.23868681183813734}, 'rouge-2': {'f': 0.011146235234880023, 'p': 0.015257121704565035, 'r': 0.014530968255534182}, 'rouge-l': {'f': 0.14552002342027948, 'p': 0.14526434798716897, 'r': 0.22030759888640442}}\n",
      "\n",
      "BLEU SCORE: 8.003946549955478e-232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustotnc/.local/lib/python3.8/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "m.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
