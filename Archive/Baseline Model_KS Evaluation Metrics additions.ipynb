{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustotnc/.pyenv/versions/3.8.5/lib/python3.8/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import unidecode\n",
    "import time, math\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Used by the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Prevent kernel from dying\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data\n",
    "labeledPfd = pd.read_csv(\"./datasets/LabeledPoetryFoundationPoems.csv\")\n",
    "\n",
    "\n",
    "# Filters the poems by category\n",
    "def get_poems_by_category(category):\n",
    "    data = []\n",
    "    for poem, emotion in zip(labeledPfd[\"poem\"], labeledPfd[\"emotion\"]):\n",
    "        if emotion == category:\n",
    "            data.append(poem)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Gets the poems in each category\n",
    "poems = {\n",
    "    \"joy\": get_poems_by_category(\"joy\"),\n",
    "    \"trust\": get_poems_by_category(\"trust\"),\n",
    "    \"sadness\": get_poems_by_category(\"sadness\"),\n",
    "    \"anticipation\": get_poems_by_category(\"anticipation\"),\n",
    "    \"fear\": get_poems_by_category(\"fear\"),\n",
    "    \"anger\": get_poems_by_category(\"anger\"),\n",
    "    \"disgust\": get_poems_by_category(\"disgust\"),\n",
    "    \"surprise\": get_poems_by_category(\"surprise\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1137379\n",
      "['Invisible', 'fish', 'swim', 'this', 'ghost', 'ocean', 'now', 'described', 'by', 'waves']\n",
      "[(['Invisible', 'fish'], 'swim'), (['fish', 'swim'], 'this'), (['swim', 'this'], 'ghost')]\n"
     ]
    }
   ],
   "source": [
    "# Training with only 100 poems.\n",
    "# When trying to train with the entrie dataset (~4000 poems)\n",
    "# the kernel dies before completing the first epoch.\n",
    "text = list(poems[\"joy\"])\n",
    "\n",
    "def joinStrings(text):\n",
    "    return '\\n'.join(string for string in text)\n",
    "text = joinStrings(text)\n",
    "\n",
    "# Clean the data\n",
    "# Removes punctuation from the dataset\n",
    "clean_data = [word for word in nltk.word_tokenize(text) if word.isalnum()]\n",
    "print(\"Number of tokens:\", len(clean_data)) # Number of tokens\n",
    "print(clean_data[:10])\n",
    "\n",
    "# Extracts the vocabulary\n",
    "vocab = set(clean_data)\n",
    "voc_len = len(vocab)\n",
    "\n",
    "# Encodes the position of each word in the vocabulary\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Generates trigram word embeddings\n",
    "# with the data. For the sentence:\n",
    "# \"I party with my friends on the weekends\"\n",
    "# \"with\", and \"my\" will be the context for\n",
    "# the word \"friends\". An example is printed bellow\n",
    "embeddings = [ ([trigram[0], trigram[1]], trigram[2]) for trigram in ngrams(clean_data, 3)]\n",
    "num_embeddings = len(embeddings)\n",
    "print(embeddings[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Input-Output Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the input and target vectors\n",
    "in_out_pairs = {\n",
    "    \"joy\": {\"inp\": [], \"tar\": []},\n",
    "    \"trust\": {\"inp\": [], \"tar\": []},\n",
    "    \"sadness\": {\"inp\": [], \"tar\": []},\n",
    "    \"anticipation\": {\"inp\": [], \"tar\": []},\n",
    "    \"fear\": {\"inp\": [], \"tar\": []},\n",
    "    \"anger\": {\"inp\": [], \"tar\": []},\n",
    "    \"disgust\": {\"inp\": [], \"tar\": []},\n",
    "    \"surprise\": {\"inp\": [], \"tar\": []}\n",
    "}\n",
    "\n",
    "# Only generates input-output pairs for the \"Joy\" dataset\n",
    "for context, target in embeddings:\n",
    "    # 2dim tensor with the positions of the context letters\n",
    "    context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "    in_out_pairs[\"joy\"][\"inp\"].append(context_idxs)\n",
    "    \n",
    "    # 1dim tensor with the position of the target letter\n",
    "    targ = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
    "    in_out_pairs[\"joy\"][\"tar\"].append(targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Recurrent Neural Network (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # We are using the GRU method to train the model\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size * 2,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            batch_first = True, # x: (num_embeddings, context_size, input_size)\n",
    "            bidirectional = False\n",
    "        )\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_vect, hidden):\n",
    "        input_vect = self.encoder(input_vect.view(1, -1)) # flattens the input vector\n",
    "        output, hidden = self.gru(input_vect.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "hidden_size = 120 # 120 nodes on each hidden layer \n",
    "n_layers = 2 # two hidden layers\n",
    "lr = 0.01 # learning rate\n",
    "batch_size = 48\n",
    "\n",
    "model = RNN(voc_len, hidden_size, voc_len, n_layers)\n",
    "model_optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Trainer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model with the data for\n",
    "# the equivalent of 1 epoch\n",
    "def train(inp, target):\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    # Initializes the gradients and the loss\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    # Trains the neural network over all\n",
    "    # the character embeddings\n",
    "    sample_data = random.sample(list(zip(inp, tar)), batch_size)\n",
    "    index = 0\n",
    "    for input_sample, output_sample in sample_data:\n",
    "        # Prints the progress every 1000th embedding\n",
    "        print(index, \"out of\", len(sample_data), end=\"\\r\")\n",
    "        \n",
    "        # The model taken in a context tensor, and\n",
    "        # the previous hidden state to predict an\n",
    "        # output, and compute a new hidden state\n",
    "        output, hidden = model.forward(input_sample, hidden)\n",
    "        \n",
    "        # The loss is computed using the predicted output\n",
    "        # and the target (expected output)\n",
    "        loss += criterion(output, output_sample)\n",
    "        \n",
    "        # Increments the index\n",
    "        index += 1\n",
    "\n",
    "    # Propagates the loss backwards\n",
    "    # through the network\n",
    "    print(\"Performing back-propagation...\")\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "\n",
    "    # Returns the loss of the network\n",
    "    return loss.data.item() / num_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing back-propagation...\n",
      "[0m 2s (1 10%) 0.0005]\n",
      "Performing back-propagation...\n",
      "[0m 2s (2 20%) 0.0005]\n",
      "Performing back-propagation...\n",
      "[0m 1s (3 30%) 0.0005]\n",
      "Performing back-propagation...\n",
      "[0m 2s (4 40%) 0.0005]\n",
      "Performing back-propagation...\n",
      "[0m 2s (5 50%) 0.0004]\n",
      "Performing back-propagation...\n",
      "[0m 2s (6 60%) 0.0004]\n",
      "Performing back-propagation...\n",
      "[0m 2s (7 70%) 0.0004]\n",
      "Performing back-propagation...\n",
      "[0m 2s (8 80%) 0.0004]\n",
      "Performing back-propagation...\n",
      "[0m 1s (9 90%) 0.0004]\n",
      "Performing back-propagation...\n",
      "[0m 2s (10 100%) 0.0004]\n",
      "\n",
      "Total Training Time: 0m 24s\n"
     ]
    }
   ],
   "source": [
    "total_time_start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "# Converts the execution time\n",
    "# to a human-readible format\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "# Trains the model.\n",
    "# n_epochs determines how many times we\n",
    "# will show the same data to the network.\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Trains the model for the\n",
    "    # current epoch\n",
    "    inp = in_out_pairs[\"joy\"][\"inp\"]\n",
    "    tar = in_out_pairs[\"joy\"][\"tar\"]\n",
    "    loss = train(inp, tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    # Logs out the epoch execution time\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "\n",
    "    # Saves the epoch execution time for later plotting\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        \n",
    "# Prints the total time taken by training the model\n",
    "print(\"\\nTotal Training Time:\", time_since(total_time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Model's Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16d63a490>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Poem Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prime_str='I love', predict_len=100, temperature=0.8):\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        prime_input = torch.tensor([word_to_ix[w] for w in prime_str.split()], dtype=torch.long)\n",
    "        inp = prime_input[-2:] #last two words as input\n",
    "        output, hidden = model.forward(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_ix.keys())[list(word_to_ix.values()).index(top_i)]\n",
    "        prime_str += \" \" + predicted_word\n",
    "\n",
    "    return prime_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love Stir impulsion elephant I simply I serious I animals a a I to fair a to a The of Confines to to to thy to a a a time of to prayer Bright to a of of thy to He night And of from of to to to a to a the to I time to to I of the its so I that a a a to to a its to like to to Traces into and to thy that a a we of to to to a have to into to our to to I of a to\n"
     ]
    }
   ],
   "source": [
    "generated_poem = generate()\n",
    "print(generated_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating with Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
     ]
    }
   ],
   "source": [
    "#score 1 sentence\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('Once perhaps it was sad, no one knows why.',\n",
    "                      'I love neighbors meadowsweet become of s s s that the on in may in the and my of Hath from crepe.')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.15492830525125167, 'p': 0.18078309305373524, 'r': 0.14215686274509803}, 'rouge-2': {'f': 0.02238988603112236, 'p': 0.02744107744107744, 'r': 0.019801980198019802}, 'rouge-l': {'f': 0.15259813467921907, 'p': 0.12307459981878585, 'r': 0.21031746031746032}}\n"
     ]
    }
   ],
   "source": [
    "#score multiple sentences - joy \n",
    "from rouge import Rouge\n",
    "\n",
    "text = list(poems[\"joy\"][:2])\n",
    "\n",
    "rouge = Rouge()\n",
    "hypotheses = [\" \".join([word for word in nltk.word_tokenize(poem) if word.isalnum()]) for poem in text]\n",
    "references = []\n",
    "\n",
    "for hyp in hypotheses:\n",
    "    first_two_words = \" \".join(hyp.split(\" \")[:2])\n",
    "    gen_poem = generate( prime_str=first_two_words )\n",
    "    references.append(\" \".join([word for word in nltk.word_tokenize(gen_poem)]))\n",
    "    \n",
    "scores = rouge.get_scores(hypotheses, references, avg=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating with Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.544641259221007e-232\n"
     ]
    }
   ],
   "source": [
    "#score multiple sentences - joy\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "references = [[word for word in nltk.word_tokenize(poem) if word.isalnum()] for poem in text]\n",
    "candidates = []\n",
    "\n",
    "for reference in references:\n",
    "    first_two_words = \" \".join(reference[:2])\n",
    "    gen_poem = generate( prime_str=first_two_words )\n",
    "    candidates.append([word for word in nltk.word_tokenize(gen_poem)])\n",
    "    \n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to the rouge and Bleu metrics, the performance of our poem generator needs to be optimized a bit more. Given that we are dealing with poetic language, the evaluation metrics we used may also not be ideal to begin with. Rouge captures the overlap between unigrams and bigrams. Since we want to generate poems that are similar to a certain emotion, overlap between bigrams does not tell us much as a metric. Bleu is a little bit better because it takes into account the structure or meaning of the sentence. This may a problem when dealing with poems because in poetic language, the structure and form of a poem may also contribute to the meaning. Furthermore, we have not accounted for rhyming schemes in the poems, which if accounted for, may improve the poems that are ultimately generated. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
